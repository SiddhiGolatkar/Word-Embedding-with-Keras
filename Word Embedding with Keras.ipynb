{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras Embedding Layer\n",
    "Keras offers an Embedding layer that can be used for neural networks on text data.\n",
    "\n",
    "\n",
    "\n",
    "It requires that the input data be integer encoded, so that each word is represented by a unique integer. This data preparation step can be performed using the Tokenizer API also provided with Keras.\n",
    "\n",
    "Embedding layer can be used:\n",
    "\n",
    "    Alone to learn a word embedding that can be saved and used in another model later.\n",
    "    As part of a deep learning model where the embedding is learned along with the model itself.\n",
    "    To load a pre-trained word embedding model, a type of transfer learning.\n",
    "\n",
    "\n",
    "Keras __Embedding__ turns positive integers (indexes) into dense vectors of fixed size. eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]. This layer can only be used as the first layer in a model.\n",
    "\n",
    "\n",
    "The Embedding layer is defined as the first hidden layer of a network. \n",
    "\n",
    "Imp Arguments:\n",
    "\n",
    "    input_dim: int > 0. Size of the vocabulary, i.e. maximum integer index + 1. e.g. if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words.\n",
    "    output_dim: int >= 0. Dimension of the dense embedding. It defines the size of the output vectors from this layer for each word.\n",
    "\n",
    "input_length: Length of input sequences. For example, if all of your input documents are comprised of 1000 words, this would be 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from numpy import zeros, asarray\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Data:\n",
    "Have 10 text documents, each with a comment about a piece of work a student submitted. Each text document is classified as positive “1” or negative “0”. This is a simple sentiment analysis problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define documents\n",
    "docs = ['Well done!',\n",
    "        'Good work',\n",
    "        'Great effort',\n",
    "        'nice work',\n",
    "        'Excellent!',\n",
    "        'Weak',\n",
    "        'Poor effort!',\n",
    "        'not good',\n",
    "        'poor work',\n",
    "        'Could have done better.']\n",
    "\n",
    "# define class labels\n",
    "labels = [1,1,1,1,1,0,0,0,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integer encode each document. This means that as input the Embedding layer will have sequences of integers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tokenizer__\n",
    "\n",
    "    Class for vectorizing texts, or/and turning texts into sequences (=list of word indexes, where the word of rank i in the dataset (starting at 1) has index i).\n",
    "\n",
    "__fit_on_texts(texts)__\n",
    "\n",
    "    Arguments:  \n",
    "        texts: list of texts to train on.\n",
    "        \n",
    "__word_index__ attribute: \n",
    "\n",
    "    Dictionary mapping words (str) to their rank/index (int). Only set after fit_on_texts was called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'work': 1, 'done': 2, 'good': 3, 'effort': 4, 'poor': 5, 'well': 6, 'great': 7, 'nice': 8, 'excellent': 9, 'weak': 10, 'not': 11, 'could': 12, 'have': 13, 'better': 14}\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# Prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "\n",
    "print (t.word_index)\n",
    "\n",
    "vocab_size = len(t.word_index) + 1\n",
    "print (vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__texts_to_sequences(texts)__\n",
    "\n",
    "    Arguments:\n",
    "        texts: list of texts to turn to sequences.\n",
    "    Return: list of sequences (one per text input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Well done!', 'Good work', 'Great effort', 'nice work', 'Excellent!', 'Weak', 'Poor effort!', 'not good', 'poor work', 'Could have done better.']\n",
      "[[6, 2], [3, 1], [7, 4], [8, 1], [9], [10], [5, 4], [11, 3], [5, 1], [12, 13, 2, 14]]\n"
     ]
    }
   ],
   "source": [
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "print(docs)\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequences have different lengths and Keras prefers inputs to be vectorized and all inputs to have the same length. We will pad all input sequences to have the length of 4. Again, we can do this with a built in Keras's pad_sequences() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6  2  0  0]\n",
      " [ 3  1  0  0]\n",
      " [ 7  4  0  0]\n",
      " [ 8  1  0  0]\n",
      " [ 9  0  0  0]\n",
      " [10  0  0  0]\n",
      " [ 5  4  0  0]\n",
      " [11  3  0  0]\n",
      " [ 5  1  0  0]\n",
      " [12 13  2 14]]\n"
     ]
    }
   ],
   "source": [
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Embedding has a vocabulary of 15 and an input length of 4. We will choose a small embedding space of 8 dimensions.\n",
    "\n",
    "The model is a simple binary classification model. \n",
    "\n",
    "Importantly, the output from the Embedding layer will be 4 vectors of 8 dimensions each, one for each word. We flatten this to a one 32-element vector to pass on to the Dense output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 4, 8)              120       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 153\n",
      "Trainable params: 153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2102a597da0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.999998\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could save the learned weights from the Embedding layer to file for later use in other models.\n",
    "\n",
    "You could also use this model generally to classify other documents that have the same kind vocabulary seen in the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pre-Trained GloVe Embedding\n",
    "\n",
    "The Keras Embedding layer can also use a word embedding learned elsewhere.\n",
    "\n",
    "It is common in the field of Natural Language Processing to learn, save, and make freely available word embeddings.\n",
    "\n",
    "For example, the researchers behind GloVe method provide a suite of pre-trained word embeddings on their website released under a public domain license.\n",
    "\n",
    "The smallest package of embeddings is 822Mb, called “glove.6B.zip“. It was trained on a dataset of one billion tokens (words) with a vocabulary of 400 thousand words. There are a few different embedding vector sizes, including 50, 100, 200 and 300 dimensions.\n",
    "\n",
    "You can download this collection of embeddings from https://nlp.stanford.edu/projects/glove/ and we can seed the Keras Embedding layer with weights from the pre-trained embedding for the words in your training dataset.\n",
    "\n",
    "After downloading and unzipping, you will see a few files, one of which is “glove.6B.100d.txt“, which contains a 100-dimensional version of the embedding.\n",
    "\n",
    "\n",
    "If you peek inside the file, you will see a token (word) followed by the weights (100 numbers) on each line. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### load the GloVe word embedding file into memory as a dictionary of word to embedding array.\n",
    "\n",
    "__Note__: Filter the embedding for the unique words in the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 14 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('glove.50d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a matrix of one embedding for each word in the training dataset. We can do that by enumerating all unique words in the Tokenizer.word_index and locating the embedding weight vector from the loaded GloVe embedding.\n",
    "\n",
    "The result is a matrix of weights only for words we will see during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 50))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 5.13589978e-01  1.96950004e-01 -5.19439995e-01 -8.62179995e-01\n",
      "   1.54940002e-02  1.09729998e-01 -8.02929997e-01 -3.33609998e-01\n",
      "  -1.61189993e-04  1.01889996e-02  4.67340015e-02  4.67510015e-01\n",
      "  -4.74750012e-01  1.10380001e-01  3.93269986e-01 -4.36520010e-01\n",
      "   3.99839997e-01  2.71090001e-01  4.26499993e-01 -6.06400013e-01\n",
      "   8.11450005e-01  4.56299990e-01 -1.27260000e-01 -2.24739999e-01\n",
      "   6.40709996e-01 -1.27670002e+00 -7.22310007e-01 -6.95900023e-01\n",
      "   2.80450005e-02 -2.30719998e-01  3.79959989e+00 -1.26249999e-01\n",
      "  -4.79669988e-01 -9.99719977e-01 -2.19760001e-01  5.05649984e-01\n",
      "   2.59530004e-02  8.05140018e-01  1.99290007e-01  2.87959993e-01\n",
      "  -1.59150004e-01 -3.04380000e-01  1.60249993e-01 -1.82899997e-01\n",
      "  -3.85629982e-02 -1.76190004e-01  2.70409994e-02  4.68420014e-02\n",
      "  -6.28970027e-01  3.57259989e-01]\n",
      " [ 3.30760002e-01 -4.38699991e-01 -3.21630001e-01 -4.93099988e-01\n",
      "   1.02540001e-01 -2.74210004e-03 -5.17199993e-01  2.43360009e-02\n",
      "  -1.28160000e-01  1.43490002e-01 -1.66909993e-01  5.61209977e-01\n",
      "  -5.62409997e-01 -4.09720019e-02  7.50000000e-01  2.30839998e-01\n",
      "   5.32040000e-01 -4.09730002e-02  2.68920004e-01 -6.92380011e-01\n",
      "   2.78829992e-01  3.79110008e-01  5.63899994e-01 -3.81500006e-01\n",
      "   7.21319973e-01 -1.35619998e+00 -8.17170024e-01 -5.48419990e-02\n",
      "   5.73329985e-01 -8.54889989e-01  3.18889999e+00  1.99180007e-01\n",
      "  -4.21200007e-01 -9.04269993e-01 -1.95209995e-01  3.01109999e-01\n",
      "   4.67559993e-01  8.21300030e-01  6.05520010e-02 -1.61430001e-01\n",
      "  -2.66680002e-01 -1.76599994e-01  1.58200003e-02  2.55279988e-01\n",
      "  -9.67390016e-02 -9.72819999e-02 -8.44829977e-02  3.33119988e-01\n",
      "  -2.22519994e-01  7.44570017e-01]\n",
      " [-3.55859995e-01  5.21300018e-01 -6.10700011e-01 -3.01310003e-01\n",
      "   9.48620021e-01 -3.15389991e-01 -5.98309994e-01  1.21880002e-01\n",
      "  -3.19430009e-02  5.56949973e-01 -1.06210001e-01  6.33989990e-01\n",
      "  -4.73399997e-01 -7.58949965e-02  3.82470012e-01  8.15690011e-02\n",
      "   8.22139978e-01  2.22200006e-01 -8.37639999e-03 -7.66200006e-01\n",
      "  -5.62529981e-01  6.17590010e-01  2.02920005e-01 -4.85979989e-02\n",
      "   8.78149986e-01 -1.65489995e+00 -7.74179995e-01  1.54349998e-01\n",
      "   9.48230028e-01 -3.95200014e-01  3.73020005e+00  8.28549981e-01\n",
      "  -1.41039997e-01  1.63950007e-02  2.11150005e-01 -3.60849984e-02\n",
      "  -1.55870005e-01  8.65830004e-01  2.63090014e-01 -7.10150003e-01\n",
      "  -3.67700011e-02  1.82819995e-03 -1.77039996e-01  2.70319998e-01\n",
      "   1.10260002e-01  1.41330004e-01 -5.73219992e-02  2.72069991e-01\n",
      "   3.13050002e-01  9.27709997e-01]\n",
      " [ 7.09020019e-01 -6.08609974e-01  2.64470011e-01 -5.45560002e-01\n",
      "  -8.00559968e-02  1.54640004e-01 -5.39529979e-01  3.18520010e-01\n",
      "   7.45949984e-01  4.50019985e-01 -6.79440022e-01 -9.34159979e-02\n",
      "  -6.25569999e-01  1.66010007e-01 -3.48589987e-01 -5.37109971e-01\n",
      "   1.00380003e+00 -5.46079993e-01  1.47909999e-01 -3.78930002e-01\n",
      "   3.24259996e-01  4.77820002e-02 -7.14290023e-01 -1.00329995e+00\n",
      "   2.08430007e-01 -1.77559996e+00  1.24679998e-01 -6.15610003e-01\n",
      "   4.94029999e-01 -6.83329999e-02  3.19689989e+00  5.07049978e-01\n",
      "  -1.09840000e+00 -6.69990003e-01 -4.27170008e-01  3.33139986e-01\n",
      "  -5.24100006e-01  2.09270000e-01 -2.67309994e-01 -9.02930021e-01\n",
      "  -4.72389996e-01 -2.15179995e-01 -1.61080003e-01 -5.66910028e-01\n",
      "  -1.50499996e-02 -2.80930009e-02  1.96119994e-01  6.29209995e-01\n",
      "  -2.38749996e-01  1.72600001e-01]\n",
      " [-5.38190007e-01 -4.07880008e-01 -1.96300000e-01 -8.08210015e-01\n",
      "   7.71450028e-02  2.57849991e-02 -5.04819989e-01 -6.88870028e-02\n",
      "   1.36280000e-01  2.86190003e-01  8.21499974e-02 -2.68860012e-01\n",
      "   3.93500000e-01 -9.48450029e-01  2.16120005e-01 -1.94330007e-01\n",
      "   1.75880000e-01 -2.83730000e-01  5.34959972e-01 -2.61350006e-01\n",
      "  -5.45120001e-01  4.91430014e-01 -3.27520013e-01 -4.06620018e-02\n",
      "   4.16889995e-01 -1.02610004e+00 -5.67520000e-02 -4.31340009e-01\n",
      "   5.06919980e-01  6.98109984e-01  3.54509997e+00  7.90690005e-01\n",
      "   7.54549980e-01 -4.28250015e-01  7.56919980e-02  3.54660004e-01\n",
      "  -3.89970005e-01 -2.14450005e-02  5.58510005e-01 -5.66009998e-01\n",
      "  -7.96440005e-01  3.87879983e-02  7.94690013e-01  3.25060010e-01\n",
      "   2.09749997e-01  2.02480003e-01 -6.31810009e-01  5.21799996e-02\n",
      "   1.07309997e+00  3.56290005e-02]\n",
      " [ 2.76910007e-01  2.87449986e-01 -2.99349993e-01 -1.99640006e-01\n",
      "   1.29559994e-01  1.55550003e-01 -6.45219982e-01 -3.40900004e-01\n",
      "  -1.18330002e-01  1.57979995e-01  1.39689997e-01  2.48720005e-01\n",
      "  -1.59009993e-01 -3.34389992e-02  1.18950002e-01  7.65350014e-02\n",
      "   4.52630013e-01  2.64939994e-01 -1.91569999e-01 -5.67680001e-01\n",
      "   2.92860009e-02  2.17449993e-01  4.34060007e-01  1.49810001e-01\n",
      "   7.57739991e-02 -1.44529998e+00 -5.83940029e-01 -4.60629985e-02\n",
      "   6.62140027e-02 -2.64169991e-01  3.96499991e+00  2.51960009e-01\n",
      "   2.48549998e-01 -5.05240023e-01  2.58060008e-01  2.86830008e-01\n",
      "  -1.79940000e-01  6.28849983e-01 -1.20399997e-01 -4.21429984e-02\n",
      "  -4.49110009e-02  1.85609996e-01  1.62660003e-01 -2.61269999e-03\n",
      "   1.30830005e-01  2.01790005e-01 -2.96669990e-01 -9.48200002e-02\n",
      "  -2.12500006e-01  2.20740009e-02]\n",
      " [-2.65670009e-02  1.33570004e+00 -1.02800000e+00 -3.72900009e-01\n",
      "   5.20120025e-01 -1.26990005e-01 -3.54330003e-01  3.78239989e-01\n",
      "  -2.97160000e-01  9.38939974e-02 -3.41220014e-02  9.29610014e-01\n",
      "  -1.40230000e-01 -6.32990003e-01  2.08010003e-02 -2.15330005e-01\n",
      "   9.69229996e-01  4.76539999e-01 -1.00390005e+00 -2.40130007e-01\n",
      "  -3.63249987e-01 -4.75700013e-03 -5.14800012e-01 -4.62599993e-01\n",
      "   1.24469995e+00 -1.83159995e+00 -1.55809999e+00 -3.74650002e-01\n",
      "   5.33620000e-01  2.08829999e-01  3.22090006e+00  6.45489991e-01\n",
      "   3.74379992e-01 -1.76569998e-01 -2.41640005e-02  3.37859988e-01\n",
      "  -4.19000000e-01  4.00810003e-01 -1.14490002e-01  5.12319990e-02\n",
      "  -1.52050003e-01  2.98550010e-01 -4.40519989e-01  1.10890001e-01\n",
      "  -2.46329993e-01  6.62509978e-01 -2.69490004e-01 -4.96580005e-01\n",
      "  -4.16180015e-01 -2.54900008e-01]\n",
      " [ 2.01890007e-01  8.06060016e-01 -1.12810004e+00 -5.95929980e-01\n",
      "   5.27559996e-01 -4.76900011e-01 -5.26400030e-01  1.45260006e-01\n",
      "  -8.60870004e-01  5.61990023e-01 -4.37079996e-01 -1.65859997e-01\n",
      "  -2.33280003e-01 -2.17260003e-01  5.21139979e-01  6.23070002e-02\n",
      "   5.51150024e-01 -1.80020005e-01 -3.29829991e-01 -9.44339991e-01\n",
      "  -6.20190024e-01  7.87639976e-01 -3.61330003e-01  6.85800016e-01\n",
      "   3.79099995e-01 -8.77439976e-01 -7.67920017e-01  1.28849995e+00\n",
      "   1.14199996e+00 -7.34889984e-01  2.39319992e+00  1.09669995e+00\n",
      "  -4.86860007e-01  5.28400004e-01  3.92699987e-01 -5.64269982e-02\n",
      "   2.96319991e-01  1.07980001e+00  4.51570004e-01 -9.81149971e-01\n",
      "   1.00370002e+00  1.56340003e-01  2.25840006e-02 -1.48320004e-01\n",
      "  -9.29329991e-02  3.36910009e-01  4.21710014e-01 -2.16419995e-01\n",
      "   1.01390004e+00  1.05350006e+00]\n",
      " [-4.04309988e-01  7.80019999e-01 -6.75379992e-01 -9.71489996e-02\n",
      "   5.42420030e-01 -5.12830019e-01 -4.77580011e-01 -1.14289999e-01\n",
      "   5.51940024e-01  4.95299995e-01  2.56810009e-01  4.15160000e-01\n",
      "   3.82230014e-01  3.39360014e-02 -5.19810021e-01 -5.75200021e-01\n",
      "   5.09750009e-01  4.40290004e-01 -1.68850005e-01 -7.56869972e-01\n",
      "  -3.67469996e-01  3.72689992e-01 -8.66999999e-02 -3.10570002e-01\n",
      "   7.03400016e-01 -4.68490005e-01 -4.78450000e-01  1.57940000e-01\n",
      "   2.39690006e-01  1.61039993e-01  2.76320004e+00  3.33810002e-01\n",
      "   4.79530007e-01 -3.73100013e-01  7.10950017e-01  6.87900007e-01\n",
      "   6.73990026e-02  1.41770005e+00 -3.56299996e-01 -4.81590003e-01\n",
      "   5.32809973e-01 -7.45930001e-02  1.60280000e-02  2.33709998e-02\n",
      "  -2.95299999e-02 -1.24629997e-01  1.25009999e-01  7.22549975e-01\n",
      "   4.75899994e-01  7.95140028e-01]\n",
      " [-2.62410015e-01 -1.11029994e+00  5.02709985e-01 -4.30519998e-01\n",
      "   3.74680012e-01 -3.05500001e-01  3.67080003e-01  2.59380013e-01\n",
      "  -1.69929996e-01  5.42450011e-01  6.39190018e-01  1.13470003e-01\n",
      "  -3.91900003e-01  3.15210015e-01 -4.29010004e-01  4.99769986e-01\n",
      "  -2.37599999e-01 -7.93070018e-01  3.44940007e-01 -4.78769988e-01\n",
      "  -5.19450009e-01 -5.06649971e-01  5.77009991e-02 -3.17970008e-01\n",
      "  -8.01339969e-02 -1.02890003e+00 -1.50700003e-01  5.09440005e-01\n",
      "   6.07150018e-01  1.30490005e+00  3.25749993e+00  1.18490003e-01\n",
      "   1.50569999e+00 -3.66490006e-01 -1.77259997e-01 -2.09309995e-01\n",
      "  -5.95269978e-01 -2.58889999e-02 -2.96499997e-01 -1.13870001e+00\n",
      "  -5.29990017e-01  6.72859997e-02  9.49539989e-02  4.97220010e-02\n",
      "   5.13230026e-01 -1.11939996e-01 -7.11099990e-03  2.37749994e-01\n",
      "   6.88740015e-01  1.38730004e-01]\n",
      " [ 5.50249994e-01 -2.49420002e-01 -9.38599987e-04 -2.63999999e-01\n",
      "   5.93200028e-01  2.79500008e-01 -2.56660014e-01  9.30759981e-02\n",
      "  -3.62879992e-01  9.07759964e-02  2.84090012e-01  7.13370025e-01\n",
      "  -4.75100011e-01 -2.44130000e-01  8.84239972e-01  8.91089976e-01\n",
      "   4.30090010e-01 -2.73299992e-01  1.12760000e-01 -8.16649973e-01\n",
      "  -4.12719995e-01  1.77540004e-01  6.19419992e-01  1.04659997e-01\n",
      "   3.33270013e-01 -2.31250000e+00 -5.23710012e-01 -2.18979996e-02\n",
      "   5.38010001e-01 -5.06150007e-01  3.86829996e+00  1.66419998e-01\n",
      "  -7.19810009e-01 -7.47280002e-01  1.16310000e-01 -3.75849992e-01\n",
      "   5.55199981e-01  1.26750007e-01 -2.26420000e-01 -1.01750001e-01\n",
      "  -3.54550004e-01  1.23480000e-01  1.65319994e-01  7.04200029e-01\n",
      "  -8.02310035e-02 -6.84060007e-02 -6.76259995e-01  3.37630004e-01\n",
      "   5.01389988e-02  3.34650010e-01]\n",
      " [ 9.07540023e-01 -3.83219987e-01  6.76479995e-01 -2.02219993e-01\n",
      "   1.51559994e-01  1.36270002e-01 -4.88130003e-01  4.82230008e-01\n",
      "  -9.57150012e-02  1.83060005e-01  2.70069987e-01  4.14150000e-01\n",
      "  -4.89329994e-01 -7.60049978e-03  7.96620011e-01  1.09889996e+00\n",
      "   5.38020015e-01 -5.44679999e-01 -1.60630003e-01 -9.83479977e-01\n",
      "  -1.91880003e-01 -2.14399993e-01  1.99589998e-01 -3.13410014e-01\n",
      "   2.41009995e-01 -2.26620007e+00 -2.59259999e-01 -1.08980000e-01\n",
      "   6.61769986e-01 -4.81040001e-01  3.62980008e+00  4.53969985e-01\n",
      "  -6.44840002e-01 -5.22440016e-01  4.29220013e-02 -1.66050002e-01\n",
      "   9.71020013e-02  4.48359996e-02  2.03889996e-01 -4.63220000e-01\n",
      "  -4.64340001e-01  3.23940009e-01  2.59840012e-01  4.08490002e-01\n",
      "   2.03510001e-01  5.87220006e-02 -1.64079994e-01  2.06719995e-01\n",
      "  -1.84400007e-01  7.11470023e-02]\n",
      " [ 9.49109972e-01 -3.49680007e-01  4.81249988e-01 -1.93059996e-01\n",
      "  -8.83840024e-03  2.81819999e-01 -9.61300015e-01 -1.35810003e-01\n",
      "  -4.30830002e-01 -9.29329991e-02  1.56890005e-01  5.95850013e-02\n",
      "  -4.96349990e-01 -1.74140006e-01  7.56609976e-01  4.92100000e-01\n",
      "   2.17730001e-01 -2.27779999e-01 -1.36859998e-01 -9.05889988e-01\n",
      "  -4.87809986e-01  1.99190006e-01  9.14470017e-01 -1.62029997e-01\n",
      "  -2.06450000e-01 -1.73119998e+00 -4.76220012e-01 -4.85399999e-02\n",
      "  -1.40269995e-01 -4.58279997e-01  4.03259993e+00  6.05199993e-01\n",
      "   1.04479998e-01 -7.36100018e-01  2.48500004e-01 -3.34610008e-02\n",
      "  -1.33949995e-01  5.27819991e-02 -2.72680014e-01  7.98249990e-02\n",
      "  -8.01270008e-01  3.08310002e-01  4.35669988e-01  8.87470007e-01\n",
      "   2.98159987e-01 -2.46500000e-02 -9.50749993e-01  3.62329990e-01\n",
      "  -7.25120008e-01 -6.08900011e-01]\n",
      " [-1.20899998e-01 -1.68210000e-01  2.40989998e-01 -3.02870005e-01\n",
      "   4.35779989e-01 -3.83670002e-01 -5.52030027e-01 -2.86810011e-01\n",
      "  -1.00919999e-01  4.77690011e-01  2.89689988e-01  2.95489997e-01\n",
      "  -4.40739989e-01 -1.34939998e-01  2.60219991e-01  4.53709990e-01\n",
      "   5.37490010e-01  6.12200014e-02  2.43660003e-01 -8.77610028e-01\n",
      "  -5.62409997e-01  2.49270007e-01  1.79409996e-01 -1.69430003e-02\n",
      "   5.79739988e-01 -1.35459995e+00 -5.31610012e-01 -2.64510006e-01\n",
      "   6.82110012e-01 -3.04820001e-01  3.79430008e+00  9.63259995e-01\n",
      "  -7.38959983e-02 -4.10320014e-01  2.44780004e-01 -1.45789996e-01\n",
      "  -8.67889971e-02  9.95000005e-01  4.99279983e-02 -6.03020012e-01\n",
      "  -3.65850002e-01 -1.01140000e-01  4.04229999e-01  2.59510010e-01\n",
      "   8.79269987e-02  6.19600005e-02  7.52660036e-02  1.27550006e-01\n",
      "   6.64609969e-02  1.11629999e+00]]\n"
     ]
    }
   ],
   "source": [
    "print (embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our model, fit, and evaluate it as before.\n",
    "\n",
    "The key difference is that the embedding layer can be seeded with the GloVe word embedding weights. \n",
    "\n",
    "    We chose the 50-dimensional version, therefore the Embedding layer must be defined with output_dim set to 50. \n",
    "    We do not want to update the learned word weights in this model, therefore we will set the trainable attribute for the model to be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, weights=[embedding_matrix], input_length=4, trainable=False))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 4, 50)             750       \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 951\n",
      "Trainable params: 201\n",
      "Non-trainable params: 750\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2102ba39ac8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ref:__\n",
    "\n",
    "    https://keras.io\n",
    "    \n",
    "    https://machinelearningmastery.com\n",
    "    \n",
    "    https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
